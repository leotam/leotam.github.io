---
layout: post
title:  "Compressive Sensing vs Deep Learning"
date:   2015-09-28 16:25:09
categories: general
---

	“In a way, residency is training the neural network of physicians”
		-- Stanford Assistant Professor of Ophthalmology Richard Chang

Recent trends in computational image analysis include compressive sensing (a topic of my thesis) and extremely popular deep learning (DL) approaches. The discussion here targets the “unreasonable effectiveness” (to misquote [Andrej Karpathy][karpathyBlog]) of deep neural networks (DNN) in practical, physician-centric approach.  A brief introduction is given for compressive sensing (CS), parallels are drawn between CS and DL, and a commentary with figures on why CS has not seen more uptake in image analysis as opposed to DL where application has been rapid (e.g. [EyePACS + Eyenuk Retinopathy diagnosis][eyeNucs] using Ben Graham’s winning [DL algorithm][benKaggle]).  The discussion focusses on human perception of texture that in a way DNNs perceives in a familiar fashion to physicians.  

Compressive sensing overview
------------
CS has an extremely robust theoretical foundation (arguably in contrast to DL) built on foundational work from Emmanuel Candes ([Google Scholar][emmScholar]), Terence Tao (2006 Fields Medalist, [blog][terryBlog]), and David Donoho ([Stanford Statistics][donohoStats]).  In short, it’s a method to exceed Nyquist sampling limits given prior information on the sparsity of the signal.  There are three requirements for CS:

1.	The signal is sparse in a transform domain.
2.	The signal is sampled incoherently in the transform domain.
3.	The signal is reconstructed using convex optimization with sparsity promoting L1 norm reconstruction.

The first point broadens the scope of the technique greatly.  If your signal is not sparse in it’s natural habit, that’s *fine*.  Just transform it to a domain where it is sparse, e.g. an NMR signal is sampled in the frequency (re: Fourier) domain and transformed to the wavelet space where biological images are sparse ([sparsity in MR images][LustigCS]).  The second point and the third point are related.  The signal is sampled incoherently in the transform domain such that aliasing propagates as noise, which is then iteratively filtered using convex optimization, 

![figure from my CS NLG paper]({{ site.url }}/assets/PSF4panelNLGpaperhires.png)
*A signal in the wavelet domain (A) is transformed to the frequency domain (B).  When is it is sampled by the probability density function in (C), the signal in the sparse domain has a noise-like background.  The background can be filtered to recover the signal.*

Exact recovery of the signal is given based on the signal sparsity and the sampling method (in [section “Optimizing the sampling method”][csNLGpaper] from my paper and citations in that section).  

Unfortunately, though the CS signal reconstruction is convergent as measured by image error, many physicians find wavelet-based CS images as “artificial”.  Namely, they lack noise-like texture that physicians perceive as features ([an example in Fig. 3][LustigCS]).  Moreover, post-injection of noise to generate familiar image characteristics is inherently undesirable and “artificial”.

Medical Features from a CNN
------------
Working with Prof. Chang on automatic diagnosis of diabetic retinopathy, he noted “In a way, residency is training the neural network of physicians.”  Our algorithm ([15th on Kaggle][kaggleLeaderboard]) used many of the techniques featured in other blog posts on the topic: common-sense data augmentation, training a deep convolutional NN on 1024x1024 images, both-eyes analysis, etc. etc.  (Unfortunately, we didn't do model ensembling due to time constraints, widely known as beneficial).

It’s possible to visualize the features extracted by the CNN and below is shown the features for a convolutional layer trained on 40000+ fundus images using DIGITS 2 ([github][digitsNV]).

![retinal features from a convolutional layer]({{ site.url }}/assets/retTexturalDIGITSv2.png)

What’s fascinating reviewing the features in this layer is that they are primarily textural/vascular.  When ophthalmologists described features indicating disease progression, the most difficult features to explain were the distributed textural features, usually corresponding to angiogenesis.  Textural features are usually foreign to human perception because we do not encounter them in everyday experience – we are not trained on these features.

Physicians view automated diagnosis tools cautiously but welcome methods to improve their accuracy and standard of care.  The right method will win out if it helps the physician.  The most promising medical applications in DL are for “pre-screen” identification and visualizations tools. Going back to Robert's quote, DL will help physicians train “smarter” by seeing more disease cases.

[karpathyBlog]:   http://karpathy.github.io/2015/05/21/rnn-effectiveness/
[eyeNucs]:        http://ir.uiowa.edu/cgi/viewcontent.cgi?article=1033&context=omia
[benKaggle]:      https://www.kaggle.com/c/diabetic-retinopathy-detection/forums/t/15801/competition-report-min-pooling-and-thank-you/89062
[emmScholar]:     https://scholar.google.com/citations?user=nRQi4O8AAAAJ&hl=en&oi=ao
[terryBlog]:      https://terrytao.wordpress.com
[donohoStats]:    https://statistics.stanford.edu/people/david-donoho
[LustigCS]:       http://onlinelibrary.wiley.com/doi/10.1002/mrm.21391/abstract
[csNLGpaper]:     http://onlinelibrary.wiley.com/doi/10.1002/mrm.25364/abstract
[kaggleLeaderboard]: https://www.kaggle.com/c/diabetic-retinopathy-detection/leaderboard
[digitsNV]:       https://github.com/NVIDIA/DIGITS
